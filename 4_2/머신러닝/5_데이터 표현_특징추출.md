
# 데이터 표현 : 특징추출.

---------------------------------------------------------------------------------------

> ## 선형변환에 의한 특징추출.

### 


### 학습의 목적.
- 분석에 불필요한 정보를 제거하고 핵심 정보만 추출.
- 차원 축소를 통한 분석 시스템의 효율 향상.

### 변환함수 embedding function, tansformation function의 종류.
- 선형변환 linear transformation.
  - n차원 열벡터 x에 변환행렬 W(nxm)을 곱하여 m차원 특징을 획득.
  - 통계적 방법으로 특징벡터 y가 원하는 분포가 되도록하는 W를 찾음.
- 비선형변환.
  - 복잡한 비선형함수 (x)를 이용하여 n차월 벡터를 m차원 벡터로 매핑.
  - 수작업 handcrafted에 의한 특징추출.
  - 표현학습 representation learning.

### 수작업에 의한 특징추출.
- 입력 데이터의 특성과 분석 목적에 맞게 특징을 개발자가 설계함.
    - 순자인식을 위한 특징.
    - 영상 분석을 위한 특징 -> 에지, 가로/세로 방향 성분 등.
    - 문서 분석을 위한 특징 -> 단어의 발생 빈도 등.

### 표현학습.
- 특징추출을 위한 비선형 변환함수를 신경망 등의 머신러닝 모델로 표현.
- 학습을 통해 분석이 잘 되도록 변환함수의 최적화가 가능.
- 예 : 딥러닝 모델을 이용한 얼굴인식용 특징추출.

### 차원 축소 관점에서의 특징추출.
- x를 W의 열벡터 wi위로 사영한 값. (단, wi는 단위벡터)

### 2차원 데이터 x를 1차원 특징 y로 변환.
- 벡터 x를 w위로 사영 -> y = (w**T)x (단, w는 단위벡터)

### 3차원 데이터 -> 2차원 특징추출.
- y는 x를 열벡터 w1,w2가 이루는 2차원 평면 위로의 사영으로 얻어지는 2차원 벡터.

### 전체 데이터 집합 X에 대한 특징추출.
- X = [x1,x2, ... , xn] (nxN 행렬)
- Y = [y1,y2, ... , yn] (mxN 행렬)

### 선형변환에 의한 특징추출.
- 주어진 데이터를 변환행렬 W에 의해 정해지는 방향으로 사영함으로 저차원의 특징값을 얻는 것.

### 좋은 특징추출이란?
- 변환행렬 W를 적절히 조절해서 분석 목적에 맞는 특징 분포를 만드는 것.
- 통계적 특징추출.

### 선형변환을 사용하는 대표적인 통계적 특징추출 방법.
1. 주성분분석법 PCA Principal Component Analysis.
    - 클래스 정보 미사용 -> 비지도 학습.
2. 선형판변분석법 LDA Linear Disciminant Analysis.
    - 클래스 정보 사용 -> 지도 학습.

---------------------------------------------------------------------------------------

> ## 주성분분석법.

### 주성분분석법.
- 목적 : 변환 전의 데이터 X가 가지고 있는 정보를 차원 축소 후에도 최대한 유지.
- 데이터 집합이 가능한 넓게 퍼질 수 있는 방향으로 사영을 수행.
- 데이터의 분산이 가장 큰 방향으로의 선형변환을 수행.
- 가장 큰 분산과 그 방향 = 공분산행렬의 최대 고유치와 고유벡터.
> 데이터의 공분산행렬의 고유치와 고유벡터를 찾아, 고유치가 가장 큰 값부터 순서대로 이에 대응하는 m개의 고유벡터를 찾아서 행렬 W를 구성.

### PCA 알고리즘의 수행 단계.
1. 입력 데이터 집합 X의 평균 ux와 공분산 ∑x를 계산.
2. 고유치 분석을 통해 공분산 ∑x의 고유치행렬 A와 고유벡터형렬 U를 계산.
3. 고유치가 큰 것부터 순서대로 m개의 고유치 {1, 2, ... n} 를 선택.
4. 선택한 고유치에 대응하는 고유벡터를 열벡터로 가지는 변환행렬 W를 생성.
5. W에 의한 선형변환으로 특징 데이터 Y를 얻음.

### PCA의 수학적 유도.
- 축소되는 차원 m을 선택하는 기준.
- 손실되는 정보량의 비중. (m개의 특징으로 표현 가능한 정보의 비율)

### 주성분분석법의 특성과 문제점.
- 데이터 분석에 대한 특별한 목적이 없는 경우에 가장 합리적인 차원 축소의 기준.
- 비지도학습.
  - 클래스 레이블 정보를 활용하지 않음 -> 분류의 핵심 정보의 손실 초래.
- 선형변환의 한계.
  - 데이터의 비선형 구조를 반영하지 못함.

---------------------------------------------------------------------------------------

> ## 선형판별분석법.

### 선형판별분석법.
- 목적 : 클래스 레이블 정보를 적극 활용.
  - 클래스 간 판별이 잘 되는 방향으로 차원 축소.

### 분류에 적합한 특징의 방향이란 ?
- 각 클래스가 가능한 서로 멀리 떨어질 수 있도록 거리를 유지.

### 선형판별분석법의 특성과 문제점.
- 지도학습 능력.
- 선형변환의 한계.
  - 복잡한 비선형 구조를 가진 경우에는 적절한 변환이 불가.
  - 커널법, 비선형 매니폴드 학습법 등을 이용한 접근 필요.
- 선택하는 고유벡터의 개수(축소된 특징 차원) m의 결정.
  - 직접 분류를 통해 얻어지는 데이터에 댛나 분류율을 기준으로 결정.
  - 행렬 (Sw**-1)Sb의해 찾아지는 고유벡터의 개수가 제한.
    - 클래스의 개수 M이면 특징 벡터는 최대 M-1차원으로 제한.
- 작은 표본집합의 문제 small sample set problem.
  - 입력 데이터 수가 입력 차원보다 작은 경우.
  - 클래스 내 산점행렬(Sw)의 역행렬이 존재하지 않음.
  - PCA로 먼저 차원 축소한 후, 이에 대한 LDA 적용.

---------------------------------------------------------------------------------------

> ## 거리 기반 차원 축소 방법.

### 기본 목적.
- 두 데이터 쌍 간의 거리를 최대한 유지하는 방향으로 차원 축소.
  - 원래 데이터 -> {x1,x2, ... ,xn}
  - 추출된 저차원의 특징 -> {y1,y2, ... , yn}
  - 원래 데이터의 거리행렬 -> D = {dij}

### 거리의 정의에 따라 다양한 방법이 존재.
1. 다차원 척도법 MDS Multi-Dimensional Scaling.
   - 유클리디안 거리 사용.
2. t-SNE.
   - 확률밀도함수를 활용하여 거리를 정의.
3. Isomap.
   - 측지 거리 geodesic distance 사용.

### 다차원 척도법.
- 거리행렬 D가 값으로 정의로 되거나 유클리디안 거리 사용.

### t-통계적 이웃 임베딩 t-Stochastic Neighbor Embedding.
- 데이터 간의 거리와 특징 간의 거리를 조건부확률을 이용한 유사도로 정의.
- 거리가 멀리 떨어진 데이터 사이의 관계를 더 잘 반영.
  
### Isomap.
- 측지 거리 사용.
- 데이터들을 정점으로 가지는 그래프 간의 경로를 데이크스트라 Dijkstra 알고리즘으로 계산.

### 거리 기반 차원 축소 방법의 특징.
- 입력 데이터와 특징 데이터 간의 매핑 함수를 정의하지 않음.
- 새로운 데이터에 대해서는 그에 대응하는 특징값을 찾을 수 없음.
- 데이터 시각화의 용도로 주로 사용.












