# 출석수업.

-------------------------------------------------------------------------------------------------------

> ## 머신러닝의 개념.

### 인공지능 AI Artificial Intelligence.
- 인간 지능을 모방하여 문제해결을 위해 사람처럼 학습/이해하는 기계를 만듦.
- 약 인공지능 weak AI.
	- 실제 지능의 소유 여부와 상관없이 지능적인 것처럼 행동하는 기계.
	- 단지 정의된 특정 목적을 달성하고 문제를 해결하는 능력.
- 강 인공지능 strong AI.
	- 지능의 모방이 아닌 실제로 인간처럼 생각하는 기계.
	- 스스로 문제 정의 및 해결, 지속적인 학습, 자아, 감정 등의 광범위한 지적 능력을 포함.

### 머신러닝 Machine Learning.
- 기계학습.
- 인간이 갖고 있는 고유의 지능적 기능인 학습 능력을 기계를 통해 구현하기 위한 접근 방법.
- 주어진 데이터를 분석하여 그로부터 일반적인 규칙이나 새로운 지식을 기계 스스로가 자동으로 추출하기 위한 접근 방법.

### 머신러닝이 필요한 이유.
- 데이터의 다양한 변형을 다루기 위해서.

### 심층 학습 deep learning.
- 심층 신경망 기반의 머신러닝 분야.

-------------------------------------------------------------------------------------------------------

> ## 머신러닝의 처리 과정.

### 머신러닝의 처리 과정.
1. 학습단계
  - 학습데이터 집합 전처리.
  - 특징추출.
  - 학습(데이터 분석)
  - 결정함수()
2. 추론단계.
  - 분류, 회귀, 군집화.
    - 테스트 데이터를 넣어서 학습단계를 거친 후에 예측 결과 추출.

### 머신러닝 시스템 개발 과정.
- 문제파악.
- 데이터 수집 및 이해.
- 데이터 준비(전처리) 특징추출. (머신러닝의 핵심)
- 모델 수립 및 학습. (머신러닝의 핵심)
- 모델 평가. (평가 오류시 문제파악부터 다시 시작)
- 배포.

-------------------------------------------------------------------------------------------------------

> ## 머신러닝의 기본 요소.

### 데이터 표현.
- n차원의 벡터.
  - 열벡터, 행벡터 모두 표현이 가능하지만 일반적으로 열벡터를 사용한다.

### 데이터 집합의 분포 특성.
- 해당 공간상에서 점들이 분포된 모양.
- 2차원 데이터 집합의 산점도.
  - 가우시안 분포.
  - 평균(3,3).
  - 공분산.
  - 데이터 개수 N = 50.

### 특징추출.
- 주어진 데이터를 처리하는 데 핵심이 되는 정보를 추출하는 것.
- 목적 -> 비용(계산량,메모리) 절약, 데이터에 포함된 불필요한 정보의 제거.

### 사영 projection에 의한 특징추출.
- 어떤 방향으로 사영하는 것이 좋은가 ?
  - 단순히 차원의 축소가 아닌 데이터 처리를 위한 핵심 정보를 추출하는 것이 더 중요.
  - 주어진 데이터의 분포 특성을 가장 잘 나타낼 수 있는 방향.

### 성능 평가.
- 학습 시스템.
  - 데이터로부터 학습을 통해 추출하고자 하는 정보를 표현하는 시스템.
  - 입력 x -> 학습 시스템 f(x) -> 출력 y.
  - 입 출력 매핑 형태의 함수로 정의.
  - 학습 -> 데이터를 이용하여 함수 f를 찾는 것 -> 학습 시스템의 매개변수 세터를 찾는 것.
  - 학습의 궁극적 목표는 앞으로 주어질 새로운 데이터에 대한 성능을 최대화하는 것.
- 목적 함수 objective function.
  - 주어진 데이터 집합을 이용하여 학습 시스템이 달성해야 하는 목표를 기게가 알 수 있는 수학적 함수로 정의한 것.
  - 오차 함수 error function.
    - 대표적인 목적함수.
    - 학습 시스템의 출력값과 원하는 출력값의 차이로 정의.
    - 학습의 목적 -> 오차를 최소화하는 것.

### 오차함수를 이용한 성능 평가 기준.
- 학습 오차 training error.
  - 학습에 사용된 데이터 집합에 대해 계산된 오차.
- 테스트 오차 test error.
  - 학습에 사용되지 않은 새로운 데잍터 집합에 대해 계산된 오차.
- 일반화 오차 generalization error.
  - 관찰될 수 있는 모든 데이터 분포 전체에 대해 정의되는 오차.
  - 실제 계산이 불가해서 테스트 오차로 대신하여 평가.

### 일반화 오차의 추정.
- 교차검증법 cross validation method.
  - 제한된 데이터 집합을 이용하여 일반화 오차에 좀 더 근접한 오차값을 얻어 내기 위한 방법.
  - K-분절 교차검증법 k-fold cross validation.
  - 교차검증오차를 계산할 수 있다.

-------------------------------------------------------------------------------------------------------

> ## 머신러닝에서의 주제.

### 머신러닝에서의 주제.
- 데이터 분석.
  - 분류 classification.
  - 회귀 regression.
  - 군집화 clustering.
- 데이터 표현.
  - 특징추출 feature extraction.

### 데이터 분석 : 분류.
- 입력 데이터가 어떤 부류에 속하는지를 자동으로 판단하는 문제.
- 예 : '~인식' -> 숫자인식, 얼굴인식, 생체인식 등.
- 베이즈 분류기, K-최근접이웃 분류기, 결정 트리 등.

### 분류 시스템의 입/출력의 관계.
- 입력과 예측출력값이 쌍으로 묶여 있어야 한다.
- 결정경계.
  - 결정 함수를 통해 얻은 값을 분류하는 경계. 
  - 결정 함수를 얻게 되면 자동으로 얻게 된다.

### 분류의 학습 목표.
- 분류 오차를 최소화하는 최적의 결정경계를 찾는 것.
- 성능 평가 척도.
- 분류율(%) = 분류 성공 데이터 개수 / 전체 데이터 개수 x 100.
- 분류오차(%) = 분류 실패 데이터 개수 / 전체 데이터 개수 x 100.

### 데이터 분석 : 회귀.
- 입력변수와 출력변수 사이의 매핑 관계를 분석.
- 예 : 시계열 예측 (시간에 따라 데이터의 변화를 분석)
- 선형회귀, 비선형회귀, 로지스틱 회귀, SVM, 신경망(MLP, RBF, CNN, LSTM)

### 회귀 시스템의 입/출력의 관계. 
- 회귀함수.

### 회귀의 학습 목표.
- 회귀 오차를 최소화하는 최적의 회귀함수를 찾는 것.
- 대표적인 예시 평균 제곱 오차 squared error.

### 데이터 분석 : 군집화.
- 데이터 집합을 서로 비슷한 몇 개의 그룹(군집 cluster)으로 묶는 문제.
  - 분류 문제와 달리 클래스 정보가 주어지지 않음.
  - 예 : 데이터 그룹화, 영상 분할.

### 군집화 시스템의 입/출력의 관계.
- 군집화 시스템의 입/출력의 관계.
- 학습 결과 -> K개의 서로소(disjoint)인 부분집합.

### 군집화의 학습 목표.
- 최적의 클러스터의 집합을 찾는 것.
- 클러스터 내의 분산은 최소화, 클러스터 간의 분산은 최대화.

### 데이터 표현 : 특징추출.
- 원래 데이터로부터 데이터 분석에 적용하기 좋은 특징을 찾아내는 문제.
- 예 : 영상 데이터의 차원 축소, 데이터 시각화.
- 주성분분석(PCA), 선형판별분석(LDA), MDS, t-SNE.

### 특징추출 시스템의 입/출력의 관계.
- 학습데이터 집합 -> 학습(데이터 분석) -> 함수 -> 테스트 데이터 -> 특징 벡터.
- 학습 결과 -> 변환함수 embedding function.

### 특칭추출 학습 목표.
- 분석 목적에 따라 달라짐.
- 예 : 차원 축소 -> 원래 데이터가 가지는 정보로부터의 손실량 최소화.

-------------------------------------------------------------------------------------------------------

> ## 학습 시스템 관련 개념.

### 머신러닝의 유형.
- 지도학습(교사학습) supervised learning.
  - 학습할 때 시스템이 출력해야 할 목표 출력값('교사')을 함께 제공.
  - 분류, 회귀.
- 비지도학습(비교사학습) unsupervised learning.
  - 학습할 때 목표 출력값에 대한 정보가 없음.
  - 군집화.
- 준지도학습(반지도학습) semi-supervised learning.
  - 지도학습 + 비지도학습 -> 클래스 레이블링 비용을 줄이려는 목적.
- 강화학습 reinforcement learning.
  - 출력값에 대한 교사 신호가 '보상 reward' 형태로 제공.
  - 교사 신호는 정확한 값이 아니고, 즉시 주어지지 않음.

### 과다적합 overfitting.
- 학습 시스템이 학습 데이터에 대해서만 지나치게 적합한 형태로 결정경게가 형성되는 현상.
- 원인 : 학습 데이터의 확률적 잡음과 학습 데이터 개수의 부족.
- 영향 : 일반화 성능 저하 초래.
- 학습 시스템의 복잡도를 조정하는 방법.
  - 다양한 변형을 가진 충분한 학습 데이터 사용.
  - 조기 종료 방법.
  - 정규항을 가진 오차함수 사용.
  - 모델 선택 방법.

-------------------------------------------------------------------------------------------------------

> ## 지도학습 : 분류.

### 데이터 분류.
- 입력 데이터를 이미 정의된 몇 개의 클래스로 구분하는 문제.

### 결정함수 g(x;세타) 를 얻는 두 가지 접근법.
1. 확률 기반 방법.
  - P(Ck|x)를 추정하여 분류.
  - 베이즈 분류기.
2. 데이터 기반 방법.
  - 데이터 간의 관계를 바탕으로 분류.
  - K-최근접이웃 분류기.

-------------------------------------------------------------------------------------------------------

> ## 베이즈 분류기.

### 베이즈 분류기 - 이진 분류 문제.
- x가 각 클래스에 속할 확률 P(C1|x), P(C2|x) 중 확률값이 큰 클래스로 할당.
- 선험확률 : 클래스가 등장할 확률. P(xn)
- 후험확률 : 각 클래스에 속할 확률. P(C1|xn)
- 우도비 likelihood ratio : 각 클래스에서 x가 관찰될 확률밀도의 비율. ( p(x|C1) / p(x|C2) )
- p(C2) / p(C1) : 전체 데이터 집합에서 각 클래스가 차지하는 비율.
- gi(xnew)가 가장 큰 클래스 i로 할당.

### 베이즈 분류기 - 다중 클래스 문제.
- 각 클래스 Ci에 대한 판별함수 -> gi(x) = p(x|Ci)P(Ci)
- 클래스 레이블 y(x)의 결정규칙 -> y(x) = argmaxi{gi(x)}

### 베이즈 분류기 처리 과정.
- 분류 절차.
  - 학습 데이터 수집.
  - 학습 데이터로부터 p(x|Ci)와 p(Ci)추정.
  - 테스트 데이터 xnew 입력.
  - 각 클래스별 판별함수 값 계산. gi(xnew) = p(x|Ci)p(Ci)
  - gi(xnew)가 가장 큰 클래스 i로 할당.

-------------------------------------------------------------------------------------------------------

> ## K-최근접이웃 분류기.

### K-최근접이웃 분류기 (K=1인 경우)
- K=1인 경우 x를 기준으로 가장 가까운 점이 어떤 클래스인지 파악해서 분류됨.
- 클래스와 상관없이 모든 데이터 중에서 가장 작ㅇ느 거리값을 갖는 데이터의 클래스로 할당.
- 최근접이웃 분류기.

### 최근접이웃 분류기. (k=1)
- 수행 단계.
  - 주어진 데이터 x와 모든 학습 데이터 {x1,x2,...,xn} 과의 거리를 계산한다.
  - 거리가 가장 가까운 데이터를 찾아 xmin으로 둔다.
  - xmin이 속하는 클래스에 할당한다.
  - 즉, y(xmin)과 같은 값을 가지도록 y(x)를 결정한다.

### 최근접이웃 분류기의 문제점.
- 과다적합 overfitting.
- k의 값을 늘려서 과다적합을 약간 해소할 수 있다.

### 최근접이웃 분류기. (K=여러개)
- 수행 단계.
  - 주어진 데이터 x와 모든 학습 데이터 {x1,x2,...,xn} 과의 거리를 계산한다.
  - 거리가 가장 가까운 것부터 순서대로 K개의 데이터를 찾아 후보 집합 N(x) = {x1,x2,...xn}을 만든다.
  - 후보 집합의 각 원소가 어떤 클래스에 속하는지 그 레이블값 y(x1),y(x2),...y(xk)을 찾는다.
  - 찾아진 레이블값 중 가장 많은 빈도수를 차지하는 클래스를 찾아 x를 그 클래스에 할당한다.

### 가우시안 베이즈 분류기.
- 각 클래스에 대한 확률분포함수를 미리 가정하고 추정.
- 학습 데이터를 통해 평균과 표준편차만 계산하여 활용.
- 분류 과정에서 학습 데이터가 불필요.

### K-최근접이웃 분류기.
- 확률분포모델을 미리 가정하지 않고 데이터 집합을 이용하여 추정.
- 새 데이터가 주어질 때마다 학습 데이터 전체와의 거리 계산이 필요.
- 항상 학습 데이터를 저장 -> 비용(계산량, 메모리) 증가.

### K-최근접이웃 분류기의 설계 고려사항.
- 적절한 K값의 결정.
  - K=1 -> 바로 이웃한 데이터에만 의존하여 클래스가 결정 -> 노이즈에 민감, 과다적합 발생.
  - K>>1 -> 주어진 데이터 주변 영역이 아닌 전체 데이터 영역에서 각 클래스가 차지하는 비율(선험확률)에 의존.
  - 주어진 데이터의 분포 특성에 의존.
    - 주어진 데이터에 대한 분류를 통해 가장 좋은 성능을 주는 값을 선택.
- 거리함수.
  - 거리함수는 주어진 데이터와 학습 데이터 간의 거리 계산.
  - 2차노름, 1차 노름, p차 노름, 내적, 코사인 거리, 정규화된 유클리디안 거리, 마할라노비스 거리 등이 있다.
  - 2차노름(유클리디안 거리)를 많이 사용한다.

### 그밖의 분류기들.
- 로지스틱 회귀.
  - 회귀 기법을 분류 문제로 확장.
- 결정 트리.
  - 속성들의 정보를 순ㅊ나적으로 적용하여 분류 -> 판단 결과에 대한 설명력이 우수.
- 서포트벡터머신(SVM)
  - 결정정계의 마진을 최대화하는 목적함수 사용 -> 일반화 성능 우수.
- 신경망(딥러닝 모델.)
  - 복잡한 결정경계를 신경망 모델로 정의하여 학습.
  - 특징추출 단계까지 한 번에 학습.










