
# 비지도학습:군집화

-------------------------------------------------------------------------------------------------------

> ## 군집화의 개념.

### 군집화.
- 데이터 집합의 분포 특성을 분석하여 서로 교차하지 않는 복수 개의 부분집합(cluster)으로 나누는 문제.
  - 입력 데이터로부터 추출된 특징 공간에서 특징값이 유사성에 따라 비슷한 데이터들끼리 묶음.
  - 입력 -> {xi}i=1,...N.
  - 비지도학습.

### 분류.
- 지도학습을 사용한다.
- 입력 -> {(xi,yi)}i=1,...N

### 군집화 실행 순서.
1. 학습 데이터 집합 xi.
2. 학습(데이터 분석)
3. 서로소인 부분집합.
4. 상황에 따라 테스트 데이터를 통한 추론을 하기도 한다.

### 군집화 결과 표현방식.
1. 샘플
2. 대표벡터.
3. 확률분포.
4. 기타 등등.

### 적용 방법론.
- K-평균 군집화.
- 계층적 군집화.
- 가우시한 혼합 모델.
- SOM Self Organizing Feature Map.

### 군집화가 적용 가능한 데이터.
- 데이터에 대한 클래스 레이블이 주어지지 않는 경우.
- 데이터에 대한 클래스 레이블링에 비용이 많이 드는 경우.

-------------------------------------------------------------------------------------------------------

> ## K-평균 군집화.

### K-평균 군집화 알고리즘.
- 주어진 데이터 집합을 K개의 그룹으로 묶는 알고리즘.
- 수행단계.
  1. 시작(초기화)
  2. 데이터 그룹핑.
  3. 대표 벡터 수정.
  4. 반복 여부 결정.

### 시작(초기화)
- 데이터 집합 {x1,x2,...xn}으로부터 임의로 K개의 벡터를 선택하여 K개의 초기 대표 벡터 집합 {m1,m2,.. mk}를 생성함.

### 데이터 그룹핑.
- Ck = { xj|d(xj,mk) <= d(xj,mi), i=1,...K }
- 각 데이터 xj(j=1,...N)에 대해 K개의 대표 벡터들과의 거리 d(xj,mk)(k=1,...,K)를 계산함.
- 만약 데이터 xj가 대표 벡터 mk에 가장 가깝다면 이 데이터를 클러스터 Ck에 속하도록 레이블링함.
- 이 과정을 통해 데이터 집합을 K개의 클러스터 {C1, C2, ... Ck}로 나눔.

### 대표 벡터 수정.
- 단계 2에서 구한 새로운 클러스터들에서 각각의 대표 벡터를 갱신함.

### 반복 여부 결정.
- 수정 전의 대표 벡터 mk와 수정 후의 대표 mknew 벡터의 차이를 계산하여 그 값에 변화가 없거나 설정된 반복 횟수에 도달할 때까지 2~4를 반복.

### 실제 문제에 적용할 떄 고려해야 할 사항.
1. 대표 벡터 계산과 데이터 그룸핑 과정의 반복적인 수행을 통해 좋은 군집을 찾는 것이 확실히 보장되는가?
2. 초기 대표 벡터의 설정이 군집화의 성능에 미치는 영향은?
3. 데이터에 의존하는 적절한 K값을 어떻게 선택할 것인가?

### 알고리즘의 특성.
1. 한 번 반복할때마다 j의 값이 줄어드는 방향으로 학습이 진행. (점차 좋은 군집을 갖게 만든다)
   - K-평균 군집화 알고리즘은 목적함수 J를 극소화하는 지역 극소점을 찾는 것을 보장.
2. 초기값에 대한 의존성 문제.
   - 초기에 임의로 결정하는 대표 벡터에 따라 최종적으로 찾아지는 해가 달라짐.
3. K값에 따른 변화. (군집의 개수)
   - 적절한 K값의 선정은 주어진 문제에 의존적.
   - 다양한 K값에 대해 군집화 결과들을 비교하여 선택.
   - 계층적 군집화 알고리즘.

-------------------------------------------------------------------------------------------------------

> ## 계층적 군집화.

### 계층적 군집화 알고리즘.
- 전체 데이터를 몇 개의 배타적인 그룹으로 나누는 대신, 큰 군집이 작은 군집을 포함하는 형태로 계층을 이루도록 군집화를 수행하여 그 구조를 살펴보는 방법.
  - 병합적 방법 agglomerative or bottom up.
    - 각 데이터가 하나의 군집을 이루는 최소 군집에서 시작하여 가까운 군집끼리 단계적으로 병합하여 더 큰 군집을 만들어 가는 방법.
    - N-1번의 병합 과정이 필요.
  - 분할적 방법 divisive or top down.
    - 모든 데이터가 하나의 군집에 속하는 최대 군집에서 시작하여 특정 기준에 따라 군집들을 분할해 가는 방법.
    - 가능한 분할 방법의 가지수 2**(n-1)-1개.

### 알고리즘 수행 단계 - 병합적 방법.
1. 데이터 집합으로부터 각 데이터가 각각의 군집이 되도록 N개의 군집을 설정함.
2. 가능한 모든 군집 쌍에 대해 군집 간의 거리를 계산함.
3. 거리가 가장 가까운 두 군집ㅈ Ci, Cj를 선택하고 병합하여 새로운 클러스터 Cij를 생성함.
4. 새로운 클러스터 Cij를 클러스터 풀에 넣고, 원래 새로운 Ci, Cj를 제거함.
5. 오직 하나의 클러스터가 남을 때까지 2~5의 과정을 반복함.

### 계층적 군집화 알고리즘의 특성.
1. 군집 간의 거리를 계산하는 방식.
   - 최단 연결법 : 가장 가까운 데이터 쌍 간의 거리.
   - 최장 연결법 : 가장 멀리 떨어진 데이터 쌍 간의 거리.
   - 중심 연결법 : 두 군집의 평균 간의 거리.
   - 평균 연결법 : 모든 데이터 쌍 간 거리의 평균.
   - Ward's 방법 : 병합 후의 클러스터 내부의 분산값.
2. 덴드로그램으로부터 적합한 군집의 수를 결정하는 방법.
   - 덴드로그램에서 클러스터 간의 거리가 증가하는 동안 클러스터의 수가 늘어나지 않고 일정 기간 유지되는 지점을 선택.

















