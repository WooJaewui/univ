
# 앙상블 학습.

--------------------------------------------------------------------------------------

> ## 앙상블 학습의 개념.

### 앙상블 학습 ensemble learning.
- 선형 분류기와 같은 간단한 학습기로 학습을 수행하지만, 복수 개의 학습기를 결합함으로써 결과적으로 더 좋은 성능을 가진 학습기를 만드는 방법.
- 고민거리.
  - 어떤 학습기를 사용할 것인지.
  - 어떻게 결합할 것인지.

### 학습기 결합에서의 고려사항.
- 학습기의 차별화 방법.
  - 학습 알고리즘의 차별화 : 접근 방법이 서로 다른 학습기 선택 -> 베이즈 분류기 & K-NN 등.
  - 모델 선택과 관련된 파라미터의 차별화 : K값이 서로 달느 복수의 K-NN, 은닉층의 뉴런 수가 서로 다른 복수의 MLP.
  - 학습 데이터의 차별화 : 같은 모델을 사용하되 학습 데이터 집합을 달리하여 복수 개의 학습기를 생성. 
- 학습기 결합 방법.
  - 병렬적 결합 : 각 학습기의 결과를 한 번에 모두 함께 고려하여 하나의 최종 결과를 생성.
  - 순차적 결합 : 각 학습기의 결과를 단계별로 결합.

### 학습 데이터 생성 방법에 따른 분류.
- 필터링 filtering에 의한 방법.
  - 각 학습기의 학습 때마다 새로운 데이터를 생성하고, 이를 이미 학습이 완료된 학습기에 적용하여 제대로 처리되지 못하는 데이터들만 필터링하여 학습.
- 리샘플링 resampling에 의한 방법.
  - 각 학습기의 학습 때마다 학습 데이터를 새로 생성하지 않고, 주어진 전체 데이터로부터 일부 집합을 추출하여 각 학습기를 학습.
- 가중치 조정 reweighting에 의한 방법.
  - 모든 학습기에 대해 동일한 학습 데이터를 사용하되, 각 데이터에 대해 가중치를 주어 학습에 대한 영향도를 조정.

--------------------------------------------------------------------------------------

> ## 배깅과 보팅.

### 배깅 bagging에 의한 학습.
- 부트스트랩 방법을 앙상블 학습에 적용한 것.
- bagging -> "bootstrap aggregating"의 약자.
- 부트스트랩 -> 제한된 데이터 집합을 이용하여 시스템의 학습과 평가를 동시에 수행하기 위한 리샘플링 기법.
- 배깅에 의한 M개의 서로 다른 학습기의 학습 과정.

### 배깅에 의한 학습 고려사항.
- 데이터 집합의 크기 ~N
  - 주어진 전체 학습 데이터 집합 X의 크기 N이 충분히 크지 않으면 ~N과 N은 같은 값으로 지정.
  - 복원추출을 사용하므로 매 단계마다 생성되는 데이터의 집합은 동일하지 않음.
- 학습에 사용될 학습기의 모델.
  - 학습기에 의해 찾아지는 판별함수가 데이터 집합의 변화에 민감한 모델을 선택하는 것이 바람직.
  - 예 : 다층 퍼셉트론, 최근접이웃 분류기 등.

### 보팅법 voting.
- M개의 학습기 결과를 모두 동일한 정도로 반영하여 평균한 결과를 얻는 방법. ("단순평균법")
- 연속된 실수값을 내야 하는 함수 근사 문제에 적합.
- 분류 문제의 경우, 결합 결과를 이용하여 치종 분류 결과를 결정해 주는 처리 과정이 필요.

--------------------------------------------------------------------------------------

> ## 부스팅.

### 부스팅 boosting.
- 간단한 학습기들이 상호보완적 역할을 할 수 있도록 단계적으로 학습을 수행하여 결합함으로써 성능을 증폭시키기 위한 방법.
- 먼저 학습된 학습기의 결과가 다음 학습기의 학습에 정보를 제공하여 이전 학습기의 결점을 보완.

### 필터링에 의한 부스팅 boosting by filtering.
- 함수 h1, h2, .. hN, 데이터 집합 x1, ... , xN이 있을 때, 학습을 시키기 위해 hN을 학습하기 위해서 h1,h2,.. h(n-1)까지 수행한다.
- h1과 h2의 결과가 일치하면 해당 결과가 최종 결과가 됨.
- 학습 데이터의 규모가 매우 커야 한다.

### AdaBoost 알고리즘.
- 같은 데이터 집합을 반복해서 사용.
- 학습할 때마다 각 데이터에 대한 가중치를 조정하여 학습의 변화를 꾀함.
- 데이터의 중요도가 적응적으로 변한다.
- 학습 방법 + 결합 방법.
  - 분류기의 중요도 -> 각 합스기의 결합 과정에서 결합계수로 사용. 

### AdaBoost 알고리즘.
- 두 개의 클래스에 대한 분류 문제에 적합 방법.

--------------------------------------------------------------------------------------

> ## 결합 방법.

### 기본적인 결합 방법.
- 대표적인 결합 방법 -> 평균법, 보팅법.
- 평균법.
  - 학습기의 출력이 수치형일 때 적합.
- 보팅법.
  - 분류 문제에서 주로 사용.
  - 출력 유형에 따른 분류.
    - 하드 보팅.
    - 소프트 보팅.

### 기본 학습기의 결과를 결합하는 학습기 사용.
- 결합기를 위한 학습 데이터 구성 방법.
  - 기본 학습기의 학습에 사용되지 않는 새로운 학습 데이터 집합을 준비.
  - 새로운 학습 데이터 (xi,yj)에 대해 기본 학습기 hj의 출력값을 zij라고 하면 결합기의 학습 데이터 -> ((z(1),z(2))...)

### 캐스캐이딩 cascading.
- 여러 복잡도를 가진 학습기들의 순차적인 결합에 중점을 둔 방법.
- 계산 시간을 줄이면서 성능도 보장.





















